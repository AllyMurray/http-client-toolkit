---
title: Recommended Usage
description: Structure your API integrations with thin wrapper modules for clean, maintainable code
---

The recommended way to use HTTP Client Toolkit is to create a **wrapper module per third-party API**. Each exported function maps to an endpoint and owns its per-request configuration. Callers never think about caching, retries, or rate limiting — they just call the function.

## Why a Wrapper?

Without a wrapper, per-request config leaks into every call site:

```typescript
// ❌ Config scattered across consumers
await client.get('https://api.github.com/users/octocat', {
  cacheTTL: 120,
  retry: { maxRetries: 2 },
});

// ... somewhere else in the codebase
await client.get('https://api.github.com/users/octocat', {
  cacheTTL: 300, // Inconsistent — which is right?
});
```

With a wrapper, config lives in one place:

```typescript
// ✅ Consumers just call the function
const user = await getUser('octocat');
```

## Example: GitHub API Wrapper

```typescript
// lib/github.ts
import { HttpClient } from '@http-client-toolkit/core';
import {
  InMemoryCacheStore,
  InMemoryDedupeStore,
  InMemoryRateLimitStore,
} from '@http-client-toolkit/store-memory';

// 1. Create one client per API with shared defaults
const client = new HttpClient({
  name: 'my-api',
  cache: new InMemoryCacheStore(),
  dedupe: new InMemoryDedupeStore(),
  rateLimit: new InMemoryRateLimitStore({
    defaultConfig: { limit: 100, windowMs: 60_000 },
  }),
  cacheTTL: 300,
  retry: { maxRetries: 2 },
  cacheOverrides: { minimumTTL: 60 },
});

// 2. Define response types
interface GitHubUser {
  login: string;
  name: string;
}

interface GitHubRepo {
  full_name: string;
  stargazers_count: number;
}

// 3. Export one function per endpoint with per-request config

/** Short-lived data — lower TTL */
export function getUser(username: string) {
  return client.get<GitHubUser>(
    `https://api.github.com/users/${username}`,
    { cacheTTL: 120 },
  );
}

/** Rarely changes — cache longer, skip no-store headers */
export function getRepo(owner: string, repo: string) {
  return client.get<GitHubRepo>(
    `https://api.github.com/repos/${owner}/${repo}`,
    {
      cacheTTL: 600,
      cacheOverrides: { ignoreNoStore: true },
    },
  );
}

/** Critical path — disable retries to fail fast */
export function getRateLimit() {
  return client.get<{ rate: { remaining: number } }>(
    'https://api.github.com/rate_limit',
    { retry: false, cacheTTL: 10 },
  );
}
```

Consumers import and call:

```typescript
import { getUser, getRepo } from './lib/github.js';

const user = await getUser('octocat');
const repo = await getRepo('octocat', 'Hello-World');
```

## Exposing Request Options

For options that genuinely vary per call site (like `signal` for cancellation or `priority` for rate limiting), pass them through:

```typescript
import { type RequestPriority } from '@http-client-toolkit/core';

interface GetUserOptions {
  signal?: AbortSignal;
  priority?: RequestPriority;
}

export function getUser(username: string, options?: GetUserOptions) {
  return client.get<GitHubUser>(
    `https://api.github.com/users/${username}`,
    {
      cacheTTL: 120,
      signal: options?.signal,
      priority: options?.priority,
    },
  );
}
```

Keep caching and retry config out of this interface — those are endpoint concerns, not caller concerns.

## Multiple APIs

Create a separate module for each API. Each gets its own client instance with appropriate store config and rate limits:

```typescript
// lib/github.ts  — 100 req/min, in-memory
// lib/stripe.ts  — 25 req/sec, SQLite for persistence
// lib/weather.ts — 60 req/min, no dedup needed
```

This keeps rate limiting isolated per origin, which matches how real APIs enforce their limits.

### Separate Stores

The simplest setup gives each client its own stores. Rate limits, cache entries, and dedup jobs are completely isolated — one API can't fill the cache or exhaust the rate limit budget of another:

```typescript
// lib/github.ts
import { HttpClient } from '@http-client-toolkit/core';
import { InMemoryCacheStore, InMemoryDedupeStore, InMemoryRateLimitStore } from '@http-client-toolkit/store-memory';

export const githubClient = new HttpClient({
  name: 'github',
  cache: new InMemoryCacheStore(),
  dedupe: new InMemoryDedupeStore(),
  rateLimit: new InMemoryRateLimitStore({
    defaultConfig: { limit: 100, windowMs: 60_000 },
  }),
});

// lib/stripe.ts
import { HttpClient } from '@http-client-toolkit/core';
import { InMemoryCacheStore, InMemoryRateLimitStore } from '@http-client-toolkit/store-memory';

export const stripeClient = new HttpClient({
  name: 'stripe',
  cache: new InMemoryCacheStore(),
  rateLimit: new InMemoryRateLimitStore({
    defaultConfig: { limit: 25, windowMs: 1_000 },
  }),
});
```

This is the right default for most applications. Each API has its own rate limits and its own cache space.

### Shared Stores

When multiple clients should share the same underlying storage — for example, a single SQLite database or a shared cache pool — pass the same store instance to each client:

```typescript
// lib/stores.ts
import { InMemoryCacheStore, InMemoryDedupeStore } from '@http-client-toolkit/store-memory';

// Shared across all clients
export const sharedCache = new InMemoryCacheStore();
export const sharedDedupe = new InMemoryDedupeStore();
```

```typescript
// lib/github.ts
import { HttpClient } from '@http-client-toolkit/core';
import { InMemoryRateLimitStore } from '@http-client-toolkit/store-memory';
import { sharedCache, sharedDedupe } from './stores.js';

export const githubClient = new HttpClient({
  name: 'github',
  cache: sharedCache,
  dedupe: sharedDedupe,
  rateLimit: new InMemoryRateLimitStore({
    defaultConfig: { limit: 100, windowMs: 60_000 },
  }),
});

// lib/stripe.ts
import { HttpClient } from '@http-client-toolkit/core';
import { InMemoryRateLimitStore } from '@http-client-toolkit/store-memory';
import { sharedCache, sharedDedupe } from './stores.js';

export const stripeClient = new HttpClient({
  name: 'stripe',
  cache: sharedCache,
  dedupe: sharedDedupe,
  rateLimit: new InMemoryRateLimitStore({
    defaultConfig: { limit: 25, windowMs: 1_000 },
  }),
});
```

Cache and dedup are shared so both clients write to and read from the same pool. Rate limiting is kept separate because each API enforces its own limits.

This pattern is especially useful with persistent backends like SQLite, where you want a single database file rather than one per client. It also means the [dashboard](/dashboard/overview/) shows the same cache data regardless of which client you select.

### When to Share

| Concern | Share? | Reasoning |
|---------|--------|-----------|
| **Cache** | Sometimes | Share when you want a single cache pool (e.g. one SQLite DB). Separate when APIs have very different TTL profiles or you want to clear one without affecting the other. |
| **Dedup** | Sometimes | Share when requests across APIs could duplicate (rare). Separate is the safer default since request hashes are already scoped by URL. |
| **Rate limit** | Rarely | Almost always separate — each API has its own rate limits. Only share if multiple clients hit the same origin and you need a combined request budget. |
